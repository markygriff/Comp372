\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{qtree}

\begin{document}

\title{COMP 372 Assignment 1}
\author{Mark Griffith}

\maketitle

% \begin{abstract}
% MARK
% \end{abstract}

\section{1.1-2}
Similarities:
\begin{enumerate}
  \item Both are graph problems
  \item	Both are optimization problems (about finding the optimal route)
  \item Both can use weighted graphs
  \item Both can use directed or undirected graphs
\end{enumerate}
Differences:
\begin{enumerate}
  \item TSP is an NP-complete problem whereas shortest-path is a known polynomial problem
  \item The solution to TSP is a cycle whereas the solution to the shortest-path is not
  \item TSP will visit at least one node twice (first and last node) whereas the shortest-path will never visit a node more than once
  \item TSP must find the permutation of every node in the graph whereas shortest
  path does not necessarily have to visit every node in the graph.
\end{enumerate}

\section{1.2-2}
For insertion sort to beat merge sort, it’s runtime must be less than that of merge sort’s. Therefore we can compare the algorithms’ runtimes as
\begin{align*}
  8n^2 &< 64nlg(n)\\
  n &< 8lg(n)
\end{align*}
Note that this problem does not have a clear-cut solution.
There are a couple of methods we can use to solve it though.
We could plug in values for n until we find an approximate value
that satisfies the above equality. Similarly, we could graph both $n$ and
$8lg(n)$ and note the value that satisfies the above equality.
Using one of these methods we see that for $n < 44$, insertion
sort beats merge sort.


\section{2.1-3}
Assuming we are 1 indexing\ldots
\begin{verbatim}
LINEAR-SEARCH(v, A):
  for i = 1 to A.length:
    if v == A[i]:
      return i
  return NULL
\end{verbatim}

\noindent
{\bfseries{\large Proof}}

\noindent
\textbf{Loop Invariant:} at the start of each for loop, $1 \leq j < i$. \\
{\textbf{Initialization:} prior to the initial for loop, there
  cannot possibly be a $j$ such that $1 \leq j < 1$ (because $i = 1$).
  Therefore, there cannot be a $A[j]$ that is equal to $v$. This
  is proof that the loop invariant holds true at initialization. \\
\textbf{Maintenance:} suppose we are on iteration $j$ of the for
  loop and that our loop invariant $A[j-1] == v$ holds true
  for the previous iteration. Then one of the two following
  is true\ldots
  \begin{enumerate}
    \item $A[j] \neq v$ (loop invariant holds) and $j$ is returned.
    \item $A[j] == v$ (loop invariant does not hold) and the loop iterates.
  \end{enumerate}
\textbf{Termination:} the conditions that causes the for loop to terminate
  are\ldots
\begin{enumerate}
  \item $i > A.length$ and so we return $NULL$ as $A[j] == v$ does
    not exist.
  \item There exists a $j$ such that $A[j] == v$ and so we return $j$
    as the loop invariant did not hold.
\end{enumerate}

\section{2.2-3}
Suppose we are searching for element x in the array.
  Assuming that each element in the array is equally likely to be x,
  there is a $\frac{1}{n}$ (where n is the length of the array) chance
  of x being any given element of the array. \\

\noindent
On average, x will be at the middle of the array.
  Therefore, the average number of elements of the array needed to be
  checked for x is $\frac{n}{2}$. \\

\noindent
In the worst case scenario (x is not in the array), all the elements of
  the array must be checked (n elements). \\

\noindent
Both cases have a run time of order $an$ where $a$ is an integer.
  Therefore, both cases have run time
$\theta(n)$.

\section{2.3-5}
\begin{verbatim}
min = A[0]
max = A[A.length - 1]
found = False

while (min <= max)
  if value = A[mid] then
    found = True
  else if value < A[mid] then
    max = mid - 1
  else
    min = mid + 1
  fi
done
\end{verbatim}

\noindent
For the above algorithm, the worst case scenario is the case in which the value
being searched for is either at the min or max element of the array A. In
that scenario, the runtime would be $\theta$(lg n) where n is the length of the
array. This is because we start with n elements in the array and for each
iteration of the loop we halve number of elements. We do this until there
only a single value left in the array. This can be represented as
\begin{align*}
  n \cdot (1/2)^x = 1
\end{align*}
where x is the number of times we need to iterate over the loop to find
the value. To solve for x, we can rearrange this to be
\begin{align*}
  n/2^x &= 1 \\
  n &= 2^x \\
  lg(n) &= x
\end{align*}

And so we see that the worst-case runtime of the binary search is
T(n) = $\theta$(lg n).
\section{3.1-1}
To prove that max(f(n), g(n)) = $\theta$(f(n) + g(n)) we must show the
  following. \\

\noindent
$\theta(f(n)) = f(n)$ : there exist positive constants $c_1, c_2,
  \text{ and } n_0$ such that $0 \leq c_1g(n) \leq f(n) \leq c_2g(n)$
  for all $n \geq n_0$, $\max(f(n)) = c_2g(n)$ \\

\noindent
We know that there must exist an $n_{0}$ such that $f(n)$ $\geq$ 0 and $g(n)$ $\geq$ 0 for all
$n$ $\geq$ $n_{0}$. So for any $n$ $\geq$ $n_{0}$, $f(n) + g(n) \geq f(n) \geq 0$ and $f(n) +
g(n) \geq g(n) \geq 0$. \\
Combining these inequalities we see that $f(n) + g(n) \geq max(f(n),g(n))$ (for $n \geq n_{0}$.
We can rewrite this as $max(f(n),g(n)) \leq c(f(n) + g(n))$ with $c = 1$. \\
Therefore, $max(f(n),g(n)) = O(f(n) + g(n))$ with $c_{1}$ = 1\\

\noindent
Now we can also write $max(f(n),g(n)) \geq f(n)$ and $max(f(n),g(n)) \geq g(n)$
for all $n \geq n_{0}$. Combining these inequalities we see that
$max(f(n),g(n)) \geq \frac{1}{2}(f(n) + g(n))$ for all $n \geq n_{0}$ \\

\noindent
Therefore, $\max(f(n),g(n)) = \Omega(f(n) + g(n))$ with $c_{2}$ = $\frac{1}{2}$ \\

\noindent
We can now see that $\max(f(n),g(n)) = \Omega(f(n) + g(n))$ bounded by $c_{1}$ = 1 and $c_{2} = \frac{1}{2}$


\section{3-1}
\textbf{a. The Asymptotic Upper Bound} \\
We must show that $p(n) = O(n^k)$ for $k \geq d$. To do this, we must find a constant $c, n_{0} \geq 0$
such that $0 \leq p(n) \leq c \cdot n^k$ for all $n \geq n_{0}$. \\
\begin{align*}
  p(n) &= \sum_{i=0}^{d} a_{i}n^i \\
       &= a_{0} + a_{1} + a_2n^2 + \dots + a_dn^d \\
       &\leq \sum_{i=0}^{d} a_i
\end{align*}
Due to the fact that $k \geq d$ for all $i$, $n^{i-k} \leq 1$ for all $n \geq 1$. \\
Now we can say that $0 \leq p(n) \leq c \cdot n^k$ with $c = \sum_{i=0}^{d} a_i$ and $n_0 = 1$. \\
Therefore $p(n) = O(n^k)$.
\\

\noindent
\textbf{b. The Asymptotic Lower Bound} \\
We must show that $p(n) = O(n^k)$ for $k \leq d$. To do this, we must find a constant $c, n_{0} \geq 0$
such that $0 \leq c \cdot n^k \leq p(n)$ for all $n \geq n_{0}$. \\
\begin{align*}
  p(n) &= \sum_{i=0}^{d} a_{i}n^i \\
       &= a_{0} + a_{1} + a_2n^2 + \dots + a_kn^k + \dots + a_dn^d \\
       &\geq a_kn^k
\end{align*}
We can see from the above that $0 \leq c \cdot n^k \leq p(n)$ with $c = a_k$ and $n_0 = 1$. \\
Therefore $p(n) = \Omega(n^k)$. \\

\noindent
\textbf{c. The Asymptotic Bound} \\
We can see from the above proofs that when $k = d$, $p(n) = O(n)$ and $p(n) = \Omega(n)$.
Therefore, $p(n) = \Theta(n)$. \\

\noindent
\textbf{d. The Asymptotic Tight Upper Bound} \\
This proof is identical to that of the Asymptotic Upper Bound without the \\
equality of $k$ and $d$.\\
We could also use the limit definition of $o$-notation as follows: \\
\begin{align*}
  \lim_{n \to \infty} \frac{p(n)}{n^k} &= 0
\end{align*}
\begin{align*}
 \lim_{n \to \infty} \frac{p(n)}{n^k} &= \lim_{n \to \infty} \frac{\sum_{i=0}^{d} a_in^i}{n^k} \\
                                      &= \lim_{n \to \infty} \frac{a_{d}n^{d}}{n^k} \\
                                      &= 0
\end{align*}
Therefore, $p(n) = o(n)$.\\

\noindent
\textbf{e. The Asymptotic Tight Lower Bound} \\
This proof is identical to that of the Asymptotic Lower Bound without the \\
equality of $k$ and $d$.\\
We could also use the limit definition of $\omega$-notation as follows: \\
\begin{align*}
  \lim_{n \to \infty} \frac{p(n)}{n^k} &= \infty
\end{align*}
\begin{align*}
 \lim_{n \to \infty} \frac{p(n)}{n^k} &= \lim_{n \to \infty} \frac{\sum_{i=0}^{d} a_in^i}{n^k} \\
                                      &= \lim_{n \to \infty} \frac{a_{d}n^{d}}{n^k} \\
                                      &= \infty
\end{align*}
Therefore, $p(n) = \omega(n)$.

\section{4.1-2}
BRUTE FORCE: find every subarray for each element in the array and keep
track of the maximum subarray
\begin{verbatim}
  MaxSubarray(array):
    total = 0
    max = 0
    for i from 0 to array.length - 1:
      total = 0
      for y from i to array.length - 1:
        total += array[y]
        if total > max:
          max = total
    return max
\end{verbatim}

\section{4.2-1}
Use Strassen's algorithm to compute the matrix product
(1 3) (6 8)
(7 5) (4 2)
Show your work

\begin{align*}
  S1 &= 8 - 2 = 6\\
  S2 &= 1 + 3 = 4\\
  S3 &= 7 + 5 = 12\\
  S4 &= 4 - 6 = -2\\
  S5 &= 1 + 5 = 6\\
  S6 &= 6 + 2 = 8\\
  S7 &= 3 - 5 = -2\\
  S8 &= 4 - 2 = 2\\
  S9 &= 1 - 7 = -6\\
  S10 &= 6 + 8 = 14
\end{align*}
\begin{align*}
  P1 &= 1 * 6 = 6\\
  P2 &= 4 * 2 = 8\\
  P3 &= 12 * 6 = 72\\
  P4 &= 5 * -2 = -10\\
  P5 &= 6 * 8 = 48\\
  P6 &= -2 * 2 = -4\\
  P7 &= -6 * 14 = -84
\end{align*}
\begin{flalign*}
  C11 &= P5 + P4 - P2 + P6 = 48 - 10 - 8  - 4 = 26\\
  C12 &= P1 + P2 = 6 + 8 = 14\\
  C21 &= P3 + P4 = 72 - 10 = 62\\
  C22 &= P5 + P1 - P3 - P7 = 48 + 6 - 72 + 84 = 66
\end{flalign*}

Therefore, according to Strassen's algorithm, the resulting matrix is
\[
\begin{bmatrix}
  C_{11} & C_{12} \\
  C_{21} & C_{22}
\end{bmatrix}
=
\begin{bmatrix}
  26 & 14 \\
  62 & 66
\end{bmatrix}
\]
\section{4.3-2}
To prove $T(n) = O(lgn)$ we must first prove that $T(n) \leq clg(n)$ for $c > 0$.
By use of substitution: \\
\begin{align*}
  T(n) &\leq clg(\lceil n/2 \rceil) + 1 \\
       &< clg(\frac{n}{2} + 1) + 1 \\
       &= clg(n + 2) - clg(2) + 1 \\
       &= clg(n + 2) - c + 1 \\
       &= clg(n) && \text{if $c \geq 1$} \\
\end{align*}
To prove that $clg(n + 2) - c + 1 \leq clg(n)$ we can find their
  derivatives when $n = 2$\ldots
\begin{align*}
  \frac{d}{dn}(clg(n + 2) - c + 1) &= \frac{c}{n+2} = \frac{c}{4}\\
  \frac{d}{dn}(clg(n)) &= \frac{c}{n} = \frac{c}{2}
\end{align*}
We can see that the rate of increase of $clg(n+2)-c+1$ is always
  less than the rate of increase of $clg(n)$ for $n = 2$
  and $c > 0$. We can also note that the two equations are equal
  when $n = 2$. Therefore we can say that $clg(n + 1) - c \leq clg(n)$.
  Hence, $T(\lceil n/2 \rceil) + 1$ is $\theta(lg(n))$ for
  $n \geq 2, c > 0$.

\section{4.4-7}
\begin{centering}
Recursion tree for $T(n) = 4T[n/2]  + cn$ \\
\end{centering}
% \begin{figure}
% \centering
\resizebox{0.9\textwidth}{!}{
\Tree [.$cn$ [.$c(n/2)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ ] [.$c(n/2)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ ] [.$c(n/2)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ ] [.$c(n/2)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ $T(n/4)$ ] ]
} \\ \\

\newpage
\begin{centering}
Cost of the tree: \\
\end{centering}
\begin{align*}
  T(n) &= \sum_{i=0}^{\lg n} 4^{i} \cdot c\frac{n}{2^{i}} \\
       &= cn \cdot \sum_{i=0}^{\lg n} 2^{i} \\
       &= cn \cdot \frac{2^{\lg n+1} - 1}{2 - 1} \\
       &= cn \cdot (2 \cdot 2^{\lg n} - 1 \\
       &= cn \cdot 2 \cdot 2n - cn \\
       &= 2cn^2 - cn \\ \\
  T(n) &= 2cn^2 - cn \\
\end{align*}
\begin{centering}
Assymptotic Bounds \\
\end{centering}
\noindent
for all n $\geq$ 2 \dots \\
upper:
\begin{align*}
  T(n) &= cn^2 + cn^2 - cn \\
  T(n) &\leq 2cn^2 \\
  T(n) &= O(n^2) \\
\end{align*}
lower:
\begin{align*}
  T(n) &= cn^2 + cn^2 - cn \\
  T(n) &\geq cn^2 \\
  T(n) &= O(n^2) \\
\end{align*}

\section{4.5-3}
Using the master method, $a = 1$ and $b = 2$. Then we have $n^{\log_{b} a} = n^0 = 1$
  and $f(n) = \theta(1) = \theta(n^{\log_{b} a}) $. This is case 2 of the master theorem.
  Therefore\ldots
\begin{align*}
  T(n) &= \theta(n^{\log_{b} a}lg(n)) \\
        &= \theta(lg(n)) \\
\end{align*}
\end{document}
